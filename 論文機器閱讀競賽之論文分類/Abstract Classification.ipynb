{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "vKzX1pMg955y",
    "outputId": "4063054c-ad52-4e0e-fd62-38f58fac5923"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "CP_aw772p-uY",
    "outputId": "09cf939b-c0b6-4f6d-924a-c3d92a6ca34e"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "JeZDBgti98i1",
    "outputId": "4b965cb5-8e9a-455d-e0e1-64af0c2fc495"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "73AF_IbU984i",
    "outputId": "a3c91470-4bc4-4a50-902e-7fb0ce137a84"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"gdrive/My Drive/Colab Notebooks/AI classify\")\n",
    "! ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAink7c7w-Hc"
   },
   "source": [
    "## Runtime Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "colab_type": "code",
    "id": "AQCJHpQSxXPX",
    "outputId": "6c712994-d8cd-416a-ae3b-8c21de2d1de3"
   },
   "outputs": [],
   "source": [
    "! pip3 install torch torchvision pandas nltk numpy sklearn tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"maxent_treebank_pos_tagger\")\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMkMf2PEw-Hh"
   },
   "source": [
    "# Take a  view of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "GZF1YwShw-Hi",
    "outputId": "dc5756ab-81b3-42be-ae12-50795442884f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('./data/task2_trainset.csv', dtype=str)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNecwUsAw-Hk"
   },
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0EJGSj_Hw-Hm"
   },
   "outputs": [],
   "source": [
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "athlGk6Fw-Hn",
    "outputId": "8c2d5923-e01a-4a0e-b240-db2788f0bfaf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dqVGBtt8w-Hq"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=890414)\n",
    "\n",
    "trainset.to_csv('trainset.csv', index=False)\n",
    "validset.to_csv('validset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jkr8-H70w-Hs"
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('./data/task2_public_testset.csv', dtype=str)\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset.to_csv('testset.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "HqHzN9mlxZny",
    "outputId": "9deedffb-53e1-45db-97d2-6d17538aeb85"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    res = []\n",
    "    for word, pos in pos_tag(word_tokenize(sentence)):\n",
    "        word = word.lower()\n",
    "        wordnet_pos = get_wordnet_pos(pos) or wordnet.NOUN\n",
    "        res.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
    "    res = [word for word in res if word not in string.punctuation]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FSTxAcQpw-Hu"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_words(data_path, n_workers=4):\n",
    "    df = pd.read_csv(data_path, dtype=str)\n",
    "    \n",
    "    sent_list = []\n",
    "    for i in df.iterrows():\n",
    "        sent_list += i[1]['Abstract'].lower().split('$$$')\n",
    "\n",
    "    words = set()\n",
    "\n",
    "    for sent in tqdm(sent_list):\n",
    "        words |= set(lemmatize_sentence(sent))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hpix1ZeAw-Hw",
    "outputId": "beb96f52-f5cb-4e09-ea94-5fd54ccc7de4"
   },
   "outputs": [],
   "source": [
    "words = set()\n",
    "# words |= collect_words('trainset.csv')\n",
    "words |= collect_words('./data/task2_trainset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dvDXc6kiw-Hy"
   },
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "word_dict = {'<pad>':PAD_TOKEN,'<unk>':UNK_TOKEN}\n",
    "for word in words:\n",
    "    word_dict[word]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JvwM9AA2w-Hz"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('dicitonary.pkl','wb') as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PT7_VSCow-H1"
   },
   "source": [
    "## Embedding class to save pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzo-O3mPw-H1"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "\n",
    "class Embedding:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        embedding_path (str): Path where embedding are loaded from (text file).\n",
    "        words (None or list): If not None, only load embedding of the words in\n",
    "            the list.\n",
    "        oov_as_unk (bool): If argument `words` are provided, whether or not\n",
    "            treat words in `words` but not in embedding file as `<unk>`. If\n",
    "            true, OOV will be mapped to the index of `<unk>`. Otherwise,\n",
    "            embedding of those OOV will be randomly initialize and their\n",
    "            indices will be after non-OOV.\n",
    "        lower (bool): Whether or not lower the words.\n",
    "        rand_seed (int): Random seed for embedding initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_path,\n",
    "                 words=None, oov_as_unk=True, lower=True, rand_seed=524):\n",
    "        self.word_dict = {}\n",
    "        self.vectors = None\n",
    "        self.lower = lower\n",
    "        self.extend(embedding_path, words, oov_as_unk)\n",
    "        torch.manual_seed(rand_seed)\n",
    "\n",
    "        if '<pad>' not in self.word_dict:\n",
    "            self.add(\n",
    "                '<pad>', torch.zeros(self.get_dim())\n",
    "            )\n",
    "        \n",
    "        if '<bos>' not in self.word_dict:\n",
    "            t_tensor = torch.rand((1, self.get_dim()), dtype=torch.float)\n",
    "            torch.nn.init.orthogonal_(t_tensor)\n",
    "            self.add(\n",
    "                '<bos>', t_tensor\n",
    "            )\n",
    "            \n",
    "        if '<eos>' not in self.word_dict:\n",
    "            t_tensor = torch.rand((1, self.get_dim()), dtype=torch.float)\n",
    "            torch.nn.init.orthogonal_(t_tensor)\n",
    "            self.add(\n",
    "                '<eos>', t_tensor\n",
    "            )\n",
    "        \n",
    "        if '<unk>' not in self.word_dict:\n",
    "            self.add('<unk>')\n",
    "\n",
    "    def to_index(self, word):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            word (str)\n",
    "\n",
    "        Return:\n",
    "             index of the word. If the word is not in `words` and not in the\n",
    "             embedding file, then index of `<unk>` will be returned.\n",
    "        \"\"\"\n",
    "        if self.lower:\n",
    "            word = word.lower()\n",
    "\n",
    "        if word not in self.word_dict:\n",
    "            return self.word_dict['<unk>']\n",
    "        else:\n",
    "            return self.word_dict[word]\n",
    "\n",
    "    def get_dim(self):\n",
    "        return self.vectors.shape[1]\n",
    "\n",
    "    def get_vocabulary_size(self):\n",
    "        return self.vectors.shape[0]\n",
    "\n",
    "    def get_dict(self):\n",
    "        return self.word_dict\n",
    "\n",
    "    def add(self, word, vector=None):\n",
    "        if self.lower:\n",
    "            word = word.lower()\n",
    "\n",
    "        if vector is not None:\n",
    "            vector = vector.view(1, -1)\n",
    "        else:\n",
    "            vector = torch.empty(1, self.get_dim())\n",
    "            torch.nn.init.uniform_(vector)\n",
    "        self.vectors = torch.cat([self.vectors, vector], 0)\n",
    "        self.word_dict[word] = len(self.word_dict)\n",
    "\n",
    "    def extend(self, embedding_path, words, oov_as_unk=True):\n",
    "        self._load_embedding(embedding_path, words)\n",
    "\n",
    "        if words is not None and not oov_as_unk:\n",
    "            # initialize word vector for OOV\n",
    "            for word in words:\n",
    "                if self.lower:\n",
    "                    word = word.lower()\n",
    "\n",
    "                if word not in self.word_dict:\n",
    "                    self.word_dict[word] = len(self.word_dict)\n",
    "\n",
    "            oov_vectors = torch.nn.init.uniform_(\n",
    "                torch.empty(len(self.word_dict) - self.vectors.shape[0],\n",
    "                            self.vectors.shape[1]))\n",
    "\n",
    "            self.vectors = torch.cat([self.vectors, oov_vectors], 0)\n",
    "\n",
    "    def _load_embedding(self, embedding_path, words):\n",
    "        if words is not None:\n",
    "            words = set(words)\n",
    "\n",
    "        vectors = []\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        with open(embedding_path, encoding=\"utf-8\") as fp:\n",
    "\n",
    "            row1 = fp.readline()\n",
    "            # if the first row is not header\n",
    "            if not re.match('^[0-9]+ [0-9]+$', row1):\n",
    "                # seek to 0\n",
    "                fp.seek(0)\n",
    "            # otherwise ignore the header\n",
    "\n",
    "            for i, line in enumerate(fp):\n",
    "                cols = line.rstrip().split(' ')\n",
    "                word = cols[0]\n",
    "                count += 1\n",
    "\n",
    "                # skip word not in words if words are provided\n",
    "                if words is not None and word not in words:\n",
    "                    continue\n",
    "                elif word not in self.word_dict:\n",
    "                    self.word_dict[word] = len(self.word_dict)\n",
    "                    vectors.append([float(v) for v in cols[1:]])\n",
    "\n",
    "        print(len(self.word_dict))\n",
    "        \n",
    "        vectors = torch.tensor(vectors)\n",
    "        if self.vectors is not None:\n",
    "            self.vectors = torch.cat([self.vectors, vectors], dim=0)\n",
    "        else:\n",
    "            self.vectors = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "t2BeLUslw-H3",
    "outputId": "638fc484-b794-44c3-c086-f6f28cfeea31"
   },
   "outputs": [],
   "source": [
    "# download Glove pretrained word embedding from web.\n",
    "print(len(words))\n",
    "# embedder = Embedding('../data/glove.840B.300d.txt', words)\n",
    "embedder = Embedding('../data/task1.model.txt', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MxeeRkO4w-H4"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('embedding.pkl','wb') as f:\n",
    "    pickle.dump(embedder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHs7kL3Pw-H7"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'THEORETICAL': 0, 'ENGINEERING':1, 'EMPIRICAL':2, 'OTHERS':3}\n",
    "    onehot = [0,0,0,0]\n",
    "    for l in labels.split():\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "        \n",
    "def sentence_to_indices(sentence, word_dict):\n",
    "    \"\"\" Convert sentence to its word indices.\n",
    "    Args:\n",
    "        sentence (str): One string.\n",
    "    Return:\n",
    "        indices (list of int): List of word indices.\n",
    "    \"\"\"\n",
    "    return [word_dict.to_index(word) for word in lemmatize_sentence(sentence)]\n",
    "    \n",
    "def get_dataset(data_path, word_dict, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch,word_dict))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset, word_dict):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1], word_dict))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data, word_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    processed = {}\n",
    "    processed['Abstract'] = [sentence_to_indices(sent, word_dict) for sent in data['Abstract'].split('$$$')]\n",
    "    if 'Task 2' in data:\n",
    "        processed['Label'] = label_to_onehot(data['Task 2'])\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "81n70z34w-H8",
    "outputId": "c12cfc3b-74e0-4e77-87b8-4682a274337d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('[INFO] Start processing trainset...')\n",
    "train = get_dataset('trainset.csv', embedder, n_workers=4)\n",
    "print('[INFO] Start processing validset...')\n",
    "valid = get_dataset('validset.csv', embedder, n_workers=4)\n",
    "print('[INFO] Start processing testset...')\n",
    "test = get_dataset('testset.csv', embedder, n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MuMr01sw-H-"
   },
   "source": [
    "## Data packing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SjNREkqQw-IA"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class AbstractDataset(Dataset):\n",
    "    def __init__(self, data, pad_idx, max_len = 500):\n",
    "        self.data = data\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_sent = max([len(data['Abstract']) for data in datas])\n",
    "        max_len = max([min(len(sentence), self.max_len) for data in datas for sentence in data['Abstract']])\n",
    "        batch_abstract = []\n",
    "        batch_label = []\n",
    "        sent_len = []\n",
    "        for data in datas:\n",
    "            # padding abstract to make them in same length\n",
    "            pad_abstract = []\n",
    "            for sentence in data['Abstract']:\n",
    "                if len(sentence) > max_len:\n",
    "                    pad_abstract.append(sentence[:max_len])\n",
    "                else:\n",
    "                    pad_abstract.append(sentence+[self.pad_idx]*(max_len-len(sentence)))\n",
    "            sent_len.append(len(pad_abstract))\n",
    "            pad_abstract.extend([[self.pad_idx]*max_len]*(max_sent-len(pad_abstract)))\n",
    "            batch_abstract.append(pad_abstract)\n",
    "            \n",
    "            # gather labels\n",
    "            if 'Label' in data:\n",
    "                batch_label.append(data['Label'])\n",
    "                \n",
    "        return torch.LongTensor(batch_abstract), torch.FloatTensor(batch_label), sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TgyxVDz-w-ID"
   },
   "outputs": [],
   "source": [
    "trainData = AbstractDataset(train, PAD_TOKEN, max_len = 2048)\n",
    "validData = AbstractDataset(valid, PAD_TOKEN, max_len = 2048)\n",
    "testData = AbstractDataset(test, PAD_TOKEN, max_len = 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yduzoOKHw-IF"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "95JiZZdPw-IH"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class simpleNet(nn.Module):\n",
    "    def __init__(self, embedder):\n",
    "        super(simpleNet, self).__init__()\n",
    "\n",
    "        self.hidden_dim = 256\n",
    "\n",
    "        self.embedding = nn.Embedding(embedder.get_vocabulary_size(), embedder.get_dim())\n",
    "        self.embedding.weight = torch.nn.Parameter(embedder.vectors)\n",
    "        self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.sent_rnn1 = nn.GRU(embedder.get_dim(),\n",
    "                                self.hidden_dim,\n",
    "                                num_layers = 2,\n",
    "                                # dropout = 0.3,\n",
    "                                bidirectional=True,\n",
    "                                batch_first=True)\n",
    "        \n",
    "        self.sent_rnn2 = nn.GRU(self.hidden_dim * 2,\n",
    "                                self.hidden_dim,\n",
    "                                num_layers = 2,\n",
    "                                # dropout = 0.5,\n",
    "                                bidirectional=True,\n",
    "                                batch_first=True)\n",
    "        \n",
    "        self.sent_dropout = nn.Dropout(0.2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        self.l1 = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n",
    "        self.l2 = nn.Linear(self.hidden_dim, 4)\n",
    "\n",
    "        nn.init.orthogonal_(self.sent_rnn1.weight_ih_l0)\n",
    "        nn.init.orthogonal_(self.sent_rnn1.weight_hh_l0)\n",
    "\n",
    "        nn.init.zeros_(self.sent_rnn1.bias_ih_l0)\n",
    "        nn.init.zeros_(self.sent_rnn1.bias_hh_l0)\n",
    "\n",
    "        nn.init.orthogonal_(self.sent_rnn2.weight_ih_l0)\n",
    "        nn.init.orthogonal_(self.sent_rnn2.weight_hh_l0)\n",
    "\n",
    "        nn.init.zeros_(self.sent_rnn2.bias_ih_l0)\n",
    "        nn.init.zeros_(self.sent_rnn2.bias_hh_l0)\n",
    "\n",
    "        nn.init.zeros_(self.l2.weight)\n",
    "        nn.init.zeros_(self.l2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        b,s,w,e = x.shape\n",
    "        x = x.view(b,s*w,e)\n",
    "        x = self.sent_dropout(x)\n",
    "        x, __ = self.sent_rnn1(x)\n",
    "        x = x.view(b,s,w,-1)\n",
    "\n",
    "        x = torch.max(x,dim=2)[0]\n",
    "\n",
    "        x, __ = self.sent_rnn2(x)\n",
    "\n",
    "        x = torch.max(x,dim=1)[0]\n",
    "\n",
    "        x = torch.relu(self.l1(x))\n",
    "\n",
    "        x = torch.sigmoid(self.l2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xxZ0_gHqw-II"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjdCmX1sw-IJ"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cquxx_Nmw-IL"
   },
   "outputs": [],
   "source": [
    "class F1():\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.3\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "        self.name = 'F1'\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "\n",
    "    def update(self, predicts, groundTruth):\n",
    "        predicts = predicts > self.threshold\n",
    "        self.n_precision += torch.sum(predicts).data.item()\n",
    "        self.n_recall += torch.sum(groundTruth).data.item()\n",
    "        self.n_corrects += torch.sum(groundTruth.type(torch.uint8) * predicts).data.item()\n",
    "\n",
    "    def get_score(self):\n",
    "        recall = self.n_corrects / self.n_recall\n",
    "        precision = self.n_corrects / (self.n_precision + 1e-20)\n",
    "        return 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "\n",
    "    def print_score(self):\n",
    "        score = self.get_score()\n",
    "        return '{:.5f}'.format(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97BCQv7Xw-IN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def _run_epoch(epoch, training):\n",
    "    model.train(training)\n",
    "    if training:\n",
    "        description = 'Train'\n",
    "        dataset = trainData\n",
    "        shuffle = True\n",
    "    else:\n",
    "        description = 'Valid'\n",
    "        dataset = validData\n",
    "        shuffle = False\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=64,\n",
    "                            shuffle=shuffle,\n",
    "                            collate_fn=dataset.collate_fn,\n",
    "                            num_workers=4)\n",
    "\n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n",
    "    loss = 0\n",
    "    f1_score = F1()\n",
    "    for i, (x, y, sent_len) in trange:\n",
    "        o_labels, batch_loss = _run_iter(x,y)\n",
    "        if training:\n",
    "            opt.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        f1_score.update(o_labels.cpu(), y)\n",
    "\n",
    "        trange.set_postfix(\n",
    "            loss=loss / (i + 1), f1=f1_score.print_score())\n",
    "    if training:\n",
    "        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "    else:\n",
    "        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "        return loss/ len(trange)\n",
    "\n",
    "def _run_iter(x,y):\n",
    "    abstract = x.to(device)\n",
    "    labels = y.to(device)\n",
    "    o_labels = model(abstract)\n",
    "    l_loss = criteria(o_labels, labels)\n",
    "    return o_labels, l_loss\n",
    "\n",
    "def save(epoch):\n",
    "    if not os.path.exists('model'):\n",
    "        os.makedirs('model')\n",
    "    torch.save(model.state_dict(), 'model/model.pkl.'+str(epoch))\n",
    "    with open('model/history.json', 'w') as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KjCWDvJtw-IO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange\n",
    "import json\n",
    "model = simpleNet(embedder)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=0.8)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(opt, 10)\n",
    "criteria = torch.nn.BCELoss()\n",
    "model.to(device)\n",
    "max_epoch = 60\n",
    "history = {'train':[],'valid':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "An0LCV3wx5Z2",
    "outputId": "f1acc1b1-06d6-4ae6-c62c-cc802a7cc247"
   },
   "outputs": [],
   "source": [
    "for epoch in range(max_epoch):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print('%e'%opt.param_groups[0]['lr'])\n",
    "    _run_epoch(epoch, True)\n",
    "    _run_epoch(epoch, False)\n",
    "    scheduler.step()\n",
    "    save(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0igsyIfw-IQ"
   },
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqmE9fxRw-IR",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model/0.6997651795'))\n",
    "model.train(False)\n",
    "_run_epoch(1, False)\n",
    "dataloader = DataLoader(dataset=testData,\n",
    "                            batch_size=128,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=testData.collate_fn,\n",
    "                            num_workers=4)\n",
    "trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n",
    "prediction = []\n",
    "for i, (x,y,_l) in trange:\n",
    "    o_labels = model(x.to(device))\n",
    "    o_labels = o_labels>0.3\n",
    "    prediction.append(o_labels.to('cpu'))\n",
    "\n",
    "prediction = torch.cat(prediction).detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5_k4nPRxw-IS"
   },
   "outputs": [],
   "source": [
    "def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        prediction (numpy array)\n",
    "        sampleFile (str)\n",
    "        public (boolean)\n",
    "        filename (str)\n",
    "    \"\"\"\n",
    "    sample = pd.read_csv(sampleFile)\n",
    "    submit = {}\n",
    "    submit['order_id'] = list(sample.order_id.values)\n",
    "    redundant = len(sample) - prediction.shape[0]\n",
    "    if public:\n",
    "        submit['THEORETICAL'] = list(prediction[:,0]) + [0]*redundant\n",
    "        submit['ENGINEERING'] = list(prediction[:,1]) + [0]*redundant\n",
    "        submit['EMPIRICAL'] = list(prediction[:,2]) + [0]*redundant\n",
    "        submit['OTHERS'] = list(prediction[:,3]) + [0]*redundant\n",
    "    else:\n",
    "        submit['THEORETICAL'] = [0]*redundant + list(prediction[:,0])\n",
    "        submit['ENGINEERING'] = [0]*redundant + list(prediction[:,1])\n",
    "        submit['EMPIRICAL'] = [0]*redundant + list(prediction[:,2])\n",
    "        submit['OTHERS'] = [0]*redundant + list(prediction[:,3])\n",
    "    df = pd.DataFrame.from_dict(submit) \n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IeZDU4zow-IU"
   },
   "outputs": [],
   "source": [
    "SubmitGenerator(prediction, \n",
    "                './task2_sample_submission.csv',\n",
    "                True, \n",
    "                './task2_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ndzCKz8w-IW"
   },
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoRz9yclw-IW"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with open('model/history.json', 'r') as f:\n",
    "    history = json.loads(f.read())\n",
    "    \n",
    "train_loss = [l['loss'] for l in history['train']]\n",
    "valid_loss = [l['loss'] for l in history['valid']]\n",
    "train_f1 = [l['f1'] for l in history['train']]\n",
    "valid_f1 = [l['f1'] for l in history['valid']]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Loss')\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(valid_loss, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('F1 Score')\n",
    "plt.plot(train_f1, label='train')\n",
    "plt.plot(valid_f1, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Best F1 score ', max([[l['f1'], idx] for idx, l in enumerate(history['valid'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ugG4wQTw-IY"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('./task2_submission.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DcaqoFUF34HS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Abstract_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
